{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwpAdX1Jpa4X"
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLxxX9sQpa4Y"
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Introduction\n",
    "- Analytical approach: normal equation\n",
    "- Iterative approach: gradient descent\n",
    "- Polynomial regression\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4RCMn4Zpa4b"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY1R56LApa4d"
   },
   "source": [
    "### Linear regression in a nutshell\n",
    "\n",
    "A linear regression model searches for a linear relationship between inputs (features) and output (target).\n",
    "\n",
    "[![Linear Regression example](https://github.com/bpesquet/mlhandbook/blob/master/algorithms/images/linear_regression.png?raw=1)](https://en.wikipedia.org/wiki/Linear_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aid7l8h9pa4g"
   },
   "source": [
    "### Problem formulation\n",
    "\n",
    "A linear model makes a prediction by computing a weighted sum of the input features, plus a constant term called *bias* (or sometimes *intercept*).\n",
    "\n",
    "$$y' = \\theta_0 + \\theta_1x_1 + \\dotsc + \\theta_nx_n$$\n",
    "\n",
    "- $y'$ is the model's output (prediction).\n",
    "- $n$ is the number of features for a data sample.\n",
    "- $x_i, i \\in [1..n]$ is the value of the $i$th feature.\n",
    "- $\\theta_i, i \\in [0..n]$ is $i$th model parameter. $\\theta_0$ is the bias term.\n",
    "\n",
    "The goal is to find the best set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiLgLrgipa4h"
   },
   "source": [
    "### Example: using a linear model to predict country happiness\n",
    "\n",
    "(Inspired by [Homemade Machine Learning](https://github.com/trekhleb/homemade-machine-learning) by Oleksii Trekhleb)\n",
    "\n",
    "The [World Happiness Report](https://www.kaggle.com/unsdsn/world-happiness#2017.csv) ranks 155 countries by their happiness levels. Several economic and social indicators (GDP, degree of freedom, level of corruption...) are recorded for each country.\n",
    "\n",
    "Can a linear model accurately predict country happiness based on these indicators ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXdbpFivpa4j"
   },
   "source": [
    "### Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NnK9_dKUpa4o",
    "outputId": "f5eeec58-778f-42d5-fc1b-74ccb06be3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.3\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9329f88e0c11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "assert platform.python_version_tuple() >= (\"3\", \"6\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9nNZEzwfpa4z",
    "outputId": "84b82ce7-3e75-4080-e977-d131c621eb10"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plotly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c19e81b0fc33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Configure Plotly to be rendered inline in the notebook.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_notebook_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plotly' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup plots\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = 10, 8\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "sns.set()\n",
    "\n",
    "# Configure Plotly to be rendered inline in the notebook.\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fVm0vBy2pa5A",
    "outputId": "bf4223d6-18ec-413b-ad7d-6d06e28bb414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 0.23.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEXNrdPvpa5Q"
   },
   "source": [
    "### Data loading and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ry8N6sqHpa5R",
    "outputId": "46e6c74b-b177-4217-8570-45fe1252dfec"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-435d3b4c6a77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load World Happiness Report for 2017\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/bpesquet/mlhandbook/master/_datasets/world-happiness-report-2017.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_happiness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print dataset shape (rows and columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load World Happiness Report for 2017\n",
    "dataset_url = \"https://raw.githubusercontent.com/bpesquet/mlhandbook/master/_datasets/world-happiness-report-2017.csv\"\n",
    "df_happiness = pd.read_csv(dataset_url)\n",
    "\n",
    "# Print dataset shape (rows and columns)\n",
    "print(f\"Dataset shape: {df_happiness.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCeH0hJGpa5Y",
    "outputId": "00d7c4a7-9f34-475c-a4bf-e81a7de2d856"
   },
   "outputs": [],
   "source": [
    "# Print a concise summary of the dataset\n",
    "df_happiness.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3FuoiLOpa5h",
    "outputId": "02ccd679-b106-4eec-86ea-04e8b31f2c3d"
   },
   "outputs": [],
   "source": [
    "# Show the 10 first samples\n",
    "df_happiness.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6ibwb8wpa5n",
    "outputId": "9a9d0ee4-5db1-4447-a57b-e9f8bb8e4d59"
   },
   "outputs": [],
   "source": [
    "# Plot histograms for all numerical attributes\n",
    "df_happiness.hist(bins=20, figsize=(16, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cMAILXTpa5t"
   },
   "source": [
    "### Univariate regression\n",
    "\n",
    "Only one feature is used by the model, which has two parameters.\n",
    "\n",
    "$$y' = \\theta_0 + \\theta_1x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkIYQ3mhpa5u"
   },
   "source": [
    "### Example: predict country happiness using only GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0M7fAR0pa5v"
   },
   "outputs": [],
   "source": [
    "def filter_dataset(df_data, input_features, target_feature):\n",
    "    \"\"\"Return a dataset containing only the selected input and output features\"\"\"\n",
    "    \n",
    "    feature_list = input_features + [target_feature]\n",
    "    return df_data[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkmEXfXIpa51"
   },
   "outputs": [],
   "source": [
    "# Define GDP as sole input feature\n",
    "input_features_uni = [\"Economy..GDP.per.Capita.\"]\n",
    "# Define country happiness as target\n",
    "target_feature = \"Happiness.Score\"\n",
    "\n",
    "df_happiness_uni = filter_dataset(df_happiness, input_features_uni, target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufsl_r6kpa57",
    "outputId": "369685fd-cf0b-4fe5-d574-b87ff2c5843e"
   },
   "outputs": [],
   "source": [
    "# Show 10 random samples\n",
    "df_happiness_uni.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11XXakFJpa6B"
   },
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26fpkzKPpa6D"
   },
   "outputs": [],
   "source": [
    "def split_dataset(df_data, input_features, target_feature):\n",
    "    \"\"\"Split dataset between training and test sets, keeping only selected features\"\"\"\n",
    "    \n",
    "    df_train, df_test = train_test_split(df_data, test_size=0.2)\n",
    "\n",
    "    print(f\"Training dataset: {df_train.shape}\")\n",
    "    print(f\"Test dataset: {df_test.shape}\")\n",
    "\n",
    "    x_train = df_train[input_features].to_numpy()\n",
    "    y_train = df_train[target_feature].to_numpy()\n",
    "\n",
    "    x_test = df_test[input_features].to_numpy()\n",
    "    y_test = df_test[target_feature].to_numpy()\n",
    "\n",
    "    print(f\"Training data: {x_train.shape}, labels: {y_train.shape}\")\n",
    "    print(f\"Test data: {x_test.shape}, labels: {y_test.shape}\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3oOpekJpa6K",
    "outputId": "2f8e7d3e-4b81-436d-a473-95999bed424c"
   },
   "outputs": [],
   "source": [
    "x_train_uni, y_train_uni, x_test_uni, y_test_uni = split_dataset(\n",
    "    df_happiness_uni, input_features_uni, target_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3V7NWlvpa6P"
   },
   "source": [
    "### Data plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2U5AVB48pa6Q",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_univariate(x, y, input_features, target_features, model_list=None):\n",
    "    \"\"\"2D plot of features and target, including model prediction if defined\"\"\"\n",
    "\n",
    "    plt.scatter(x, y, label=\"Actual\")\n",
    "\n",
    "    if model_list is not None:\n",
    "        predictions_count = 100\n",
    "        x_pred = np.linspace(x.min(), x.max(), predictions_count).reshape(\n",
    "            predictions_count, 1\n",
    "        )\n",
    "        for model_name, model in model_list.items():\n",
    "            y_pred = model.predict(x_pred)\n",
    "            plt.plot(x_pred, y_pred, \"r\", label=model_name)\n",
    "\n",
    "    plt.xlabel(input_features)\n",
    "    plt.ylabel(target_feature)\n",
    "    plt.title(\"Countries Happiness\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpxuFnGQpa6W",
    "outputId": "1b0063c0-abbc-46bb-ebcb-cdd424c160f5"
   },
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "plot_univariate(x_train_uni, y_train_uni, input_features_uni, target_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB30KLVRpa6c"
   },
   "source": [
    "## Analytical approach: normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBsx8Bsepa6c"
   },
   "source": [
    "### Problem formulation\n",
    "\n",
    "- $\\pmb{x}^{(i)}$: $i$th data sample, vector of $n+1$ features $x^{(i)}_j$ with $x^{(i)}_0 = 1$.\n",
    "- $\\pmb{\\theta}$: parameters of the linear model, vector of $n+1$ values $\\theta_j$.\n",
    "- $\\mathcal{h}_\\theta$: hypothesis function (relationship between inputs and targets).\n",
    "- $y'^{(i)}$: model output for the $i$th sample.\n",
    "\n",
    "$$\\pmb{x}^{(i)} = \\begin{pmatrix}\n",
    "       \\ x^{(i)}_0 \\\\\n",
    "       \\ x^{(i)}_1 \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ x^{(i)}_n\n",
    "     \\end{pmatrix} \\in \\pmb{R}^{n+1}\n",
    "\\;\\;\\;\n",
    "\\pmb{\\theta} = \\begin{pmatrix}\n",
    "       \\ \\theta_0 \\\\\n",
    "       \\ \\theta_1 \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\theta_n\n",
    "     \\end{pmatrix} \\in \\pmb{R}^{n+1}$$\n",
    "\n",
    "$$y'^{(i)} = \\mathcal{h}_\\theta(x^{(i)}) = \\theta_0 + \\theta_1x^{(i)}_1 + \\dotsc + \\theta_nx^{(i)}_n = \\pmb{\\theta}^T\\pmb{x}^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JB7j0wbpa6d"
   },
   "source": [
    "### Loss function\n",
    "\n",
    "We use the *Mean Squared Error* (MSE). RMSE is also a possible choice.\n",
    "\n",
    "$$\\mathcal{L}(\\pmb{\\theta}) = \\frac{1}{m}\\sum_{i=1}^m (y'^{(i)} - y^{(i)})^2 = \\frac{1}{m}\\sum_{i=1}^m (\\mathcal{h}_\\theta(x^{(i)}) - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1I2VjEVzpa6f"
   },
   "source": [
    "### Analytical solution\n",
    "\n",
    "- Technique for computing the regression coefficients $\\theta_i$ analytically (by calculus).\n",
    "- One-step learning algorithm (no iterations).\n",
    "- Also called *Ordinary Least Squares*.\n",
    "\n",
    "$$\\pmb{\\theta^{*}} = (\\pmb{X}^T\\pmb{X})^{-1}\\pmb{X}^T\\pmb{y}$$\n",
    "\n",
    "- $\\pmb{\\theta^*}$ is the parameter vector that minimizes the loss function $\\mathcal{L}(\\theta)$.\n",
    "- This result is called the **Normal Equation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjLvQ2r0pa6g"
   },
   "source": [
    "### Math proof (optional)\n",
    "\n",
    "(Inspired by [Derivation of the Normal Equation for linear regression](https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/) and [ML From Scratch, Part 1: Linear Regression](http://www.oranlooney.com/post/ml-from-scratch-part-1-linear-regression/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiCXl8-npa6h"
   },
   "source": [
    "#### Vectorized notation\n",
    "\n",
    "- $\\pmb{X}$: matrix of input data (*design matrix*). Each line corresponds to a sample.\n",
    "- $\\pmb{y}$: vector of target values.\n",
    "\n",
    "$$\\pmb{X} = \\begin{bmatrix}\n",
    "       \\ x^{(0)T} \\\\\n",
    "       \\ x^{(1)T} \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ x^{(m)T} \\\\\n",
    "     \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "       \\ x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_n \\\\\n",
    "       \\ x^{(2)}_0 & x^{(2)}_1 & \\cdots & x^{(2)}_n \\\\\n",
    "       \\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       \\ x^{(m)}_0 & x^{(m)}_1 & \\cdots & x^{(m)}_n\n",
    "     \\end{bmatrix}\\;\\;\\;\n",
    "\\pmb{y} = \\begin{pmatrix}\n",
    "       \\ y^{(1)} \\\\\n",
    "       \\ y^{(2)} \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ y^{(m)}\n",
    "     \\end{pmatrix}$$\n",
    "     \n",
    "$$\\pmb{X}\\pmb{\\theta} =\n",
    "\\begin{pmatrix}\n",
    "       \\ \\theta_0 + \\theta_1x^{(1)}_1 + \\dotsc + \\theta_nx^{(1)}_n \\\\\n",
    "       \\ \\theta_0 + \\theta_1x^{(2)}_1 + \\dotsc + \\theta_nx^{(2)}_n \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\theta_0 + \\theta_1x^{(m)}_1 + \\dotsc + \\theta_nx^{(m)}_n\n",
    "     \\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "       \\ \\mathcal{h}_\\theta(x^{(1)}) \\\\\n",
    "       \\ \\mathcal{h}_\\theta(x^{(2)}) \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\mathcal{h}_\\theta(x^{(m)})\n",
    "     \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0I2YLlupa6h"
   },
   "source": [
    "#### Vectorized loss\n",
    "\n",
    "The loss can also be expressed using a vectorized notation.\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{m}\\sum_{i=1}^m (\\mathcal{h}_\\theta(x^{(i)}) - y^{(i)})^2 = \\frac{1}{m}{{\\lVert{\\pmb{X}\\pmb{\\theta} - \\pmb{y}}\\rVert}_2}^2$$\n",
    "\n",
    "The squared norm of a vector $\\pmb{v}$ is the inner product of that vector with its transpose: $\\sum_{i=1}^n v_i^2 = \\pmb{v}^T \\pmb{v}$.\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{m}(\\pmb{X}\\pmb{\\theta} - \\pmb{y})^T(\\pmb{X}\\pmb{\\theta} - \\pmb{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqKNPWsqpa6j"
   },
   "source": [
    "The previous expression can be developped.\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{m}\\left((\\pmb{X}\\pmb{\\theta})^T - \\pmb{y}^T)(\\pmb{X}\\pmb{\\theta} - \\pmb{y}\\right) = \\frac{1}{m}\\left((\\pmb{X}\\pmb{\\theta})^T\\pmb{X}\\pmb{\\theta} - (\\pmb{X}\\pmb{\\theta})^T\\pmb{y} - \\pmb{y}^T(\\pmb{X}\\pmb{\\theta}) + \\pmb{y}^T\\pmb{y}\\right)$$\n",
    "\n",
    "Since $\\pmb{X}\\pmb{\\theta}$ and $\\pmb{y}$ are vectors, $(\\pmb{X}\\pmb{\\theta})^T\\pmb{y} = \\pmb{y}^T(\\pmb{X}\\pmb{\\theta})$.\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{m}\\left(\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta} - 2(\\pmb{X}\\pmb{\\theta})^T\\pmb{y} + \\pmb{y}^T\\pmb{y}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbI6Bz7cpa6k"
   },
   "source": [
    "#### Loss gradient\n",
    "\n",
    "We must find the $\\pmb{\\theta^*}$ vector that minimizes the loss function $\\mathcal{L}(\\theta)$.\n",
    "\n",
    "$$\\pmb{\\theta^*} = \\underset{\\theta}{\\mathrm{argmin}}\\;\\mathcal{L}(\\theta)$$\n",
    "\n",
    "Since the loss function is continuous, convex and differentiable everywhere (in simplest termes: bowl-shaped), it admits one unique global minimum, for which the gradient vector $\\nabla_{\\theta}\\mathcal{L}(\\pmb{\\theta})$ is equal to $\\vec{0}$.\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathcal{L}(\\pmb{\\theta}) = \\begin{pmatrix}\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_0} \\mathcal{L}(\\boldsymbol{\\theta}) \\\\\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_1} \\mathcal{L}(\\boldsymbol{\\theta}) \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_n} \\mathcal{L}(\\boldsymbol{\\theta})\n",
    "     \\end{pmatrix} = \\nabla_{\\theta}\\left(\\frac{1}{m}(\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta} - 2(\\pmb{X}\\pmb{\\theta})^T\\pmb{y} + \\pmb{y}^T\\pmb{y})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q-chQ55pa6o"
   },
   "source": [
    "#### Computation of loss gradient terms\n",
    "\n",
    "Since $\\pmb{y}^T\\pmb{y}$ is constant w.r.t. $\\pmb{\\theta}$, $\\nabla_{\\theta}(\\pmb{y}^T\\pmb{y}) = \\vec{0}$.\n",
    "\n",
    "$$2(\\pmb{X}\\pmb{\\theta})^T\\pmb{y} = 2\\;\\begin{pmatrix}\n",
    "       \\ \\theta_0 + \\theta_1x^{(1)}_1 + \\dotsc + \\theta_nx^{(1)}_n \\\\\n",
    "       \\ \\theta_0 + \\theta_1x^{(2)}_1 + \\dotsc + \\theta_nx^{(2)}_n \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\theta_0 + \\theta_1x^{(m)}_1 + \\dotsc + \\theta_nx^{(m)}_n\n",
    "     \\end{pmatrix}^T\\begin{pmatrix}\n",
    "       \\ y^{(1)} \\\\\n",
    "       \\ y^{(2)} \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ y^{(m)}\n",
    "     \\end{pmatrix} = 2\\sum_{i=1}^m y^{(i)}(\\theta_0 + \\theta_1x^{(i)}_1 + \\dotsc + \\theta_nx^{(i)}_n)$$\n",
    "     \n",
    "Reminder: $\\forall i \\in [1..m], x_0^{(i)} = 1$.\n",
    "\n",
    "$$2(\\pmb{X}\\pmb{\\theta})^T\\pmb{y} =2\\sum_{i=1}^m y^{(i)}\\sum_{j=0}^n x_j^{(i)}\\theta_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvrZZFggpa6o"
   },
   "source": [
    "$$\\nabla_{\\theta}\\left(2(\\pmb{X}\\pmb{\\theta})^T\\pmb{y}\\right) = \n",
    "\\begin{pmatrix}\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_0} \\left(2(\\pmb{X}\\pmb{\\theta})^T\\pmb{y}\\right)  \\\\\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_1} \\left(2(\\pmb{X}\\pmb{\\theta})^T\\pmb{y}\\right) \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_n} \\left(2(\\pmb{X}\\pmb{\\theta})^T\\pmb{y}\\right)\n",
    "     \\end{pmatrix} =\n",
    "2\\begin{pmatrix}\n",
    "       \\ \\sum_{i=1}^m y^{(i)}x_0^{(i)} \\\\\n",
    "       \\ \\sum_{i=1}^m y^{(i)}x_1^{(i)} \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\sum_{i=1}^m y^{(i)}x_n^{(i)}\n",
    "     \\end{pmatrix} = 2 \\pmb{X}^T\\pmb{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki5Q3o_npa6q"
   },
   "source": [
    "$\\pmb{X}^T\\pmb{X}$ is a square and symmetric matrix called $\\pmb{A}$ here for simplicity of notation.\n",
    "\n",
    "$$\\pmb{X}^T\\pmb{X} = \\begin{bmatrix}\n",
    "       \\ x^{(1)}_0 & x^{(2)}_0 & \\cdots & x^{(m)}_0 \\\\\n",
    "       \\ x^{(1)}_1 & x^{(2)}_1 & \\cdots & x^{(m)}_1 \\\\\n",
    "       \\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       \\ x^{(1)}_n & x^{(2)}_n & \\cdots & x^{(m)}_n\n",
    "     \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "       \\ x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_n \\\\\n",
    "       \\ x^{(2)}_0 & x^{(2)}_1 & \\cdots & x^{(2)}_n \\\\\n",
    "       \\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       \\ x^{(m)}_0 & x^{(m)}_1 & \\cdots & x^{(m)}_n\n",
    "     \\end{bmatrix} = \\pmb{A} \\in \\pmb{R}^{n+1 \\times n+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLmcgAq0pa6r"
   },
   "source": [
    "$$\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta} = \\begin{pmatrix}\n",
    "       \\ \\theta_0 \\\\\n",
    "       \\ \\theta_1 \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\theta_n\n",
    "     \\end{pmatrix}^T\n",
    "     \\begin{bmatrix}\n",
    "       \\ a_{00} & a_{01} & \\cdots & a_{0n} \\\\\n",
    "       \\ a_{10} & a_{11} & \\cdots & a_{1n} \\\\\n",
    "       \\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       \\ a_{n0} & a_{n1} & \\cdots & a_{nn}\n",
    "     \\end{bmatrix}\n",
    "     \\begin{pmatrix}\n",
    "       \\ \\theta_0 \\\\\n",
    "       \\ \\theta_1 \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\theta_n\n",
    "     \\end{pmatrix} = \n",
    "     \\begin{pmatrix}\n",
    "       \\ \\theta_0 \\\\\n",
    "       \\ \\theta_1 \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\theta_n\n",
    "     \\end{pmatrix}^T\n",
    "     \\begin{pmatrix}\n",
    "       \\ a_{00}\\theta_0 + a_{01}\\theta_1 + \\dotsc + a_{0n}\\theta_n \\\\\n",
    "       \\ a_{10}\\theta_0 + a_{11}\\theta_1 + \\dotsc + a_{1n}\\theta_n \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ a_{n0}\\theta_0 + a_{n1}\\theta_1 + \\dotsc + a_{nn}\\theta_n\n",
    "     \\end{pmatrix}$$\n",
    "     \n",
    "$$\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta} = \\theta_0(a_{00}\\theta_0 + a_{01}\\theta_1 + \\dotsc + a_{0n}\\theta_n) + \\theta_1(a_{10}\\theta_0 + a_{11}\\theta_1 + \\dotsc + a_{1n}\\theta_n) + \\dotsc + \\theta_n(a_{n0}\\theta_0 + a_{n1}\\theta_1 + \\dotsc + a_{nn}\\theta_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOZ9o4jopa6r"
   },
   "source": [
    "$$\\frac{\\partial}{\\partial \\theta_0} \\left(\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta}\\right) =\n",
    "2a_{00}\\theta_0 + a_{01}\\theta_1 + \\dotsc + a_{0n}\\theta_n + a_{10}\\theta_1 + a_{20}\\theta_2 + \\dotsc + a_{n0}\\theta_n$$\n",
    "\n",
    "Since $\\pmb{A}$ is symmetric, $\\forall i,j \\in [1..n,1..n], a_{ij} = a_{ji}$.\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_0} \\left(\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta}\\right) =\n",
    "2(a_{00}\\theta_0 + a_{01}\\theta_1 + \\dotsc + a_{0n}\\theta_n) =\n",
    "2\\sum_{j=0}^n a_{0j}\\theta_j$$\n",
    "\n",
    "$$\\nabla_{\\theta}\\left(\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta}\\right)=\n",
    "\\begin{pmatrix}\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_0} \\left(\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta}\\right)  \\\\\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_1} \\left(\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta}\\right) \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_n} \\left(\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta}\\right)\n",
    "     \\end{pmatrix} =\n",
    "     2\\begin{pmatrix}\n",
    "       \\ \\sum_{j=0}^n a_{0j}\\theta_j  \\\\\n",
    "       \\ \\sum_{j=0}^n a_{1j}\\theta_j \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\sum_{j=0}^n a_{nj}\\theta_j\n",
    "     \\end{pmatrix}=\n",
    "     2\\pmb{A}\\pmb{\\theta} = 2\\pmb{X}^T\\pmb{X}\\pmb{\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU9ud3aYpa6s"
   },
   "source": [
    "#### Final gradient expression\n",
    "\n",
    "We can finally express the gradient of the loss function w.r.t. the model parameters:\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathcal{L}(\\pmb{\\theta}) = \\nabla_{\\theta}\\left(\\frac{1}{m}(\\pmb{\\theta}^T\\pmb{X}^T\\pmb{X}\\pmb{\\theta} - 2(\\pmb{X}\\pmb{\\theta})^T\\pmb{y} + \\pmb{y}^T\\pmb{y})\\right) = \\frac{1}{m}\\left(2\\pmb{X}^T\\pmb{X}\\pmb{\\theta} - 2\\pmb{X}^T\\pmb{y}\\right)$$\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathcal{L}(\\pmb{\\theta}) = \\frac{2}{m}\\pmb{X}^T\\left(\\pmb{X}\\pmb{\\theta} - \\pmb{y}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyyNE76ipa6t"
   },
   "source": [
    "#### Loss minimization\n",
    "\n",
    "The $\\pmb{\\theta^*}$ vector that minimizes the loss is such as the gradient is equal to $\\vec{0}$. In other terms:\n",
    "\n",
    "$$\\pmb{X}^T\\pmb{X}\\pmb{\\theta^{*}} - \\pmb{X}^T\\pmb{y} = \\vec{0}$$\n",
    "\n",
    "$$\\pmb{X}^T\\pmb{X}\\pmb{\\theta^{*}} = \\pmb{X}^T\\pmb{y}$$\n",
    "\n",
    "If $\\pmb{X}^T\\pmb{X}$ is an inversible matrix, the result is given by:\n",
    "\n",
    "$$\\pmb{\\theta^{*}} = (\\pmb{X}^T\\pmb{X})^{-1}\\pmb{X}^T\\pmb{y}$$\n",
    "\n",
    "Which is exactly the Normal Equation we were expecting to see!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xbT0ivEpa6t"
   },
   "source": [
    "### Example: applying Normal Equation to predict country happiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGHLyBgypa6v"
   },
   "outputs": [],
   "source": [
    "def train_model(model, x, y):\n",
    "    model.fit(x, y)\n",
    "    print(f\"Model weights: {model.coef_}, bias: {model.intercept_}\")\n",
    "\n",
    "    error = mean_squared_error(y, model.predict(x))\n",
    "    print(f\"Training error: {error:.05f}\")\n",
    "\n",
    "\n",
    "def test_model(model, x, y):\n",
    "    error = mean_squared_error(y, model.predict(x))\n",
    "    print(f\"Test error: {error:.05f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2m4bR0U2pa60",
    "outputId": "80b221c1-404a-4cd0-8b50-dfee5295e582"
   },
   "outputs": [],
   "source": [
    "# Create a Linear Regression model (based on Normal Equation)\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Train and test the model on univariate data\n",
    "train_model(lr_model, x_train_uni, y_train_uni)\n",
    "test_model(lr_model, x_test_uni, y_test_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrN8bAOmpa65",
    "outputId": "27a0c9ee-bd7c-4ead-8e6a-6a3f3aeac9ad"
   },
   "outputs": [],
   "source": [
    "# Plot data and model prediction\n",
    "plot_univariate(\n",
    "    x_train_uni,\n",
    "    y_train_uni,\n",
    "    input_features_uni,\n",
    "    target_feature,\n",
    "    model_list={\"LR\": lr_model},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhsRMqokpa6_"
   },
   "source": [
    "### Multivariate regression\n",
    "\n",
    "General case: several features are used by the model.\n",
    "\n",
    "$$y' = \\theta_0 + \\theta_1x_1 + \\dotsc + \\theta_nx_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8YdM1iepa7A",
    "outputId": "49a0ab96-231a-4990-830c-2f739420ebb1"
   },
   "outputs": [],
   "source": [
    "# Using two input features: GDP and degree of freedom\n",
    "input_features_multi = [\"Economy..GDP.per.Capita.\", \"Freedom\"]\n",
    "\n",
    "x_train_multi, y_train_multi, x_test_multi, y_test_multi = split_dataset(\n",
    "    df_happiness, input_features_multi, target_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfHm_OFjpa7D",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_multivariate(x, y, input_features, target_features, model=None):\n",
    "    \"\"\"3D plot of features and target, including model prediction if defined\"\"\"\n",
    "    \n",
    "    # Configure the plot with training dataset\n",
    "    plot_training_trace = go.Scatter3d(\n",
    "        x=x[:, 0].flatten(),\n",
    "        y=x[:, 1].flatten(),\n",
    "        z=y.flatten(),\n",
    "        name=\"Actual\",\n",
    "        mode=\"markers\",\n",
    "        marker={\n",
    "            \"size\": 10,\n",
    "            \"opacity\": 1,\n",
    "            \"line\": {\"color\": \"rgb(255, 255, 255)\", \"width\": 1},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    plot_data = plot_training_trace\n",
    "\n",
    "    if model is not None:\n",
    "        # Generate different combinations of X and Y sets to build a predictions plane.\n",
    "        predictions_count = 10\n",
    "\n",
    "        # Find min and max values along X and Y axes.\n",
    "        x_min = x[:, 0].min()\n",
    "        x_max = x[:, 0].max()\n",
    "\n",
    "        y_min = x[:, 1].min()\n",
    "        y_max = x[:, 1].max()\n",
    "\n",
    "        # Generate predefined numbe of values for eaxh axis betwing correspondent min and max values.\n",
    "        x_axis = np.linspace(x_min, x_max, predictions_count)\n",
    "        y_axis = np.linspace(y_min, y_max, predictions_count)\n",
    "\n",
    "        # Create empty vectors for X and Y axes predictions\n",
    "        # We're going to find cartesian product of all possible X and Y values.\n",
    "        x_pred = np.zeros((predictions_count * predictions_count, 1))\n",
    "        y_pred = np.zeros((predictions_count * predictions_count, 1))\n",
    "\n",
    "        # Find cartesian product of all X and Y values.\n",
    "        x_y_index = 0\n",
    "        for x_index, x_value in enumerate(x_axis):\n",
    "            for y_index, y_value in enumerate(y_axis):\n",
    "                x_pred[x_y_index] = x_value\n",
    "                y_pred[x_y_index] = y_value\n",
    "                x_y_index += 1\n",
    "\n",
    "        # Predict Z value for all X and Y pairs.\n",
    "        z_pred = model.predict(np.hstack((x_pred, y_pred)))\n",
    "\n",
    "        # Plot training data with predictions.\n",
    "\n",
    "        # Configure the plot with test dataset.\n",
    "        plot_predictions_trace = go.Scatter3d(\n",
    "            x=x_pred.flatten(),\n",
    "            y=y_pred.flatten(),\n",
    "            z=z_pred.flatten(),\n",
    "            name=\"Prediction Plane\",\n",
    "            mode=\"markers\",\n",
    "            marker={\"size\": 1,},\n",
    "            opacity=0.8,\n",
    "            surfaceaxis=2,\n",
    "        )\n",
    "\n",
    "        plot_data = [plot_training_trace, plot_predictions_trace]\n",
    "\n",
    "    # Configure the layout.\n",
    "    plot_layout = go.Layout(\n",
    "        title=\"Date Sets\",\n",
    "        scene={\n",
    "            \"xaxis\": {\"title\": input_features[0]},\n",
    "            \"yaxis\": {\"title\": input_features[1]},\n",
    "            \"zaxis\": {\"title\": target_feature},\n",
    "        },\n",
    "        margin={\"l\": 0, \"r\": 0, \"b\": 0, \"t\": 0},\n",
    "    )\n",
    "\n",
    "    plot_figure = go.Figure(data=plot_data, layout=plot_layout)\n",
    "\n",
    "    # Render 3D scatter plot.\n",
    "    plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izM7gIIopa7H",
    "outputId": "a813acbf-9a34-487b-c5d8-6b67894ce3d6"
   },
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "plot_multivariate(x_train_multi, y_train_multi, input_features_multi, target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0W0vyubUpa7P",
    "outputId": "161e5ee7-8308-4d8e-d564-8317ff4b8e60"
   },
   "outputs": [],
   "source": [
    "# Create a Linear Regression model (based on Normal Equation)\n",
    "lr_model_multi = LinearRegression()\n",
    "\n",
    "# Train and test the model on multivariate data\n",
    "train_model(lr_model_multi, x_train_multi, y_train_multi)\n",
    "test_model(lr_model_multi, x_test_multi, y_test_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SU3xjz_Zpa7U",
    "outputId": "30563664-beb2-45c7-e1b5-bb22c9a3c0d9"
   },
   "outputs": [],
   "source": [
    "# Plot data and model prediction\n",
    "plot_multivariate(x_train_multi, y_train_multi, input_features_multi, target_feature, lr_model_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZVlH5ZLpa7Z"
   },
   "source": [
    "### Pros/cons of analytical approach\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Computed in one step.\n",
    "- Exact solution.\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Computation of $(\\pmb{X}^T\\pmb{X})^{-1}$ is slow when the number of features is large ($n > 10^4$).\n",
    "- Doesn't work if $\\pmb{X}^T\\pmb{X}$ is not inversible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVyPri4opa7a"
   },
   "source": [
    "## Iterative approach: gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5mufZvLpa7b"
   },
   "source": [
    "### Method description\n",
    "\n",
    "- Same objective: find the $\\pmb{\\theta}^{*}$ vector that minimizes the loss.\n",
    "- General idea:\n",
    "  - Start with random values for $\\pmb{\\theta}$.\n",
    "  - Update $\\pmb{\\theta}$ in small steps towards loss minimization.\n",
    "- To know in which direction update $\\pmb{\\theta}$, we compute the **gradient** of the loss function w.r.t. $\\pmb{\\theta}$ and go into the opposite direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4G4nyZvpa7c"
   },
   "source": [
    "### Computation of gradients\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathcal{L}(\\pmb{\\theta}) = \\begin{pmatrix}\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_0} \\mathcal{L}(\\theta) \\\\\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_1} \\mathcal{L}(\\theta) \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_n} \\mathcal{L}(\\theta)\n",
    "     \\end{pmatrix} =\n",
    "\\frac{2}{m}\\pmb{X}^T\\left(\\pmb{X}\\pmb{\\theta} - \\pmb{y}\\right)$$\n",
    "\n",
    "(See math proof above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gxedPdIpa7c"
   },
   "source": [
    "### Parameters update\n",
    "\n",
    "The **_learning rate_** $\\eta$ governs the amplitude of updates.\n",
    "\n",
    "$$\\pmb{\\theta}_{next} = \\pmb{\\theta} - \\eta\\nabla_{\\theta}\\mathcal{L}(\\pmb{\\theta})$$\n",
    "\n",
    "[![Importance of learning rate](https://github.com/bpesquet/mlhandbook/blob/master/algorithms/images/learning_rate.png?raw=1)](https://www.jeremyjordan.me/nn-learning-rate/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHCMmU-xpa7d"
   },
   "source": [
    "### Interactive example\n",
    "\n",
    "[![Gradient descent line graph](https://github.com/bpesquet/mlhandbook/blob/master/algorithms/images/gradient_descent_line_graph.gif?raw=1)](https://alykhantejani.github.io/a-brief-introduction-to-gradient-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWmENTRcpa7e"
   },
   "source": [
    "### Gradient descent types\n",
    "\n",
    "- *Batch*: use the whole dataset to compute gradients.\n",
    "  - Safe but potentially slow.\n",
    "- *Stochastic*: use only one sample.\n",
    "  - Fast but potentially erratic.\n",
    "- *Mini-Batch*: use a small set of samples (10-1000).\n",
    "  - Good compromise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diheWKYSpa7f"
   },
   "source": [
    "### Example: applying stochastic gradient descent to predict country happiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFD11V4Ipa7f",
    "outputId": "b29deb24-00dd-4ecd-adc7-c7e9c3f0688b"
   },
   "outputs": [],
   "source": [
    "# Create a Linear Regression model (based on Stochastic Gradient Descent)\n",
    "sgd_model_uni = SGDRegressor()\n",
    "\n",
    "# Train and test the model on univariate data\n",
    "train_model(sgd_model_uni, x_train_uni, y_train_uni)\n",
    "test_model(sgd_model_uni, x_test_uni, y_test_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWZYZ8_gpa7i",
    "outputId": "691461c2-cfea-4021-d121-92a885d5625b"
   },
   "outputs": [],
   "source": [
    "# Plot data and models predictions\n",
    "plot_univariate(\n",
    "    x_train_uni,\n",
    "    y_train_uni,\n",
    "    input_features_uni,\n",
    "    target_feature,\n",
    "    model_list={\"LR\": lr_model, \"SGD\": sgd_model_uni},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NUTtZ-Zpa7m"
   },
   "source": [
    "### Pros/cons of iterative approach\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Works well with a large number of features.\n",
    "- MSE loss function convex => guarantee of a global minimum.\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Convergence depends on learning rate and GD type.\n",
    "- Dependant on feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0TLz5LEpa7n"
   },
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICab2c4qpa7n"
   },
   "source": [
    "### General idea\n",
    "\n",
    "- How can a linear model fit non-linear data?\n",
    "- Solution: add powers of each feature as new features.\n",
    "- The hypothesis function is still linear.\n",
    "- High-degree polynomial regression can be subject to severe overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S-kvL5Lpa7o"
   },
   "source": [
    "### Example: fitting a quadratic curve with polynomial regression\n",
    "\n",
    "(Heavily inspired by Chapter 4 of [Hands-On Machine Learning](https://github.com/ageron/handson-ml2) by Aurélien Géron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OBYfdCOTpa7p"
   },
   "outputs": [],
   "source": [
    "# Generate quadratic data with noise\n",
    "# (ripped from https://github.com/ageron/handson-ml2)\n",
    "m = 200\n",
    "x_quad = 6 * np.random.rand(m, 1) - 3\n",
    "# y = 0,5x2 + x + 2 + noise\n",
    "y_quad = 0.5 * x_quad**2 + x_quad + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nrmKeduIpa7r",
    "outputId": "7beaf11d-a476-419a-c77e-4d077a4d2b16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features for first sample: [-1.69245515]\n",
      "New features for first sample: [-1.69245515  2.86440443]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial features for first sample: {x_quad[0]}\")\n",
    "\n",
    "# Add polynomial features to the dataset\n",
    "poly_degree = 2\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "x_quad_poly = poly_features.fit_transform(x_quad)\n",
    "\n",
    "print(f\"New features for first sample: {x_quad_poly[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GSngcb2kpa7y",
    "outputId": "d955eb0e-4670-434f-dbb6-830296aac82a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights: [[1.03498999 0.49453146]], bias: [2.00374519]\n"
     ]
    }
   ],
   "source": [
    "# Fit a linear regression model to the extended data\n",
    "lr_model_poly = LinearRegression()\n",
    "lr_model_poly.fit(x_quad_poly, y_quad)\n",
    "\n",
    "# Should be close to [1, 0,5] and 2\n",
    "print(f\"Model weights: {lr_model_poly.coef_}, bias: {lr_model_poly.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ki_I3JkLpa72",
    "outputId": "39dfa4d7-26fc-42e3-bb32-d117afc331df"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data and model prediction\n",
    "plt.plot(x_quad, y_quad, \"b.\", label=\"Data\")\n",
    "x_pred = np.linspace(-3, 3, m).reshape(m, 1)\n",
    "x_pred_poly = poly_features.transform(x_pred)\n",
    "y_pred = lr_model_poly.predict(x_pred_poly)\n",
    "plt.plot(x_pred, y_pred, \"r-\", linewidth=2, label=\"Prediction\")\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlIUpNpVpa74"
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LlnFa1Gpa75"
   },
   "source": [
    "### General idea\n",
    "\n",
    "- Solution against overfitting: constraining model parameters.\n",
    "\n",
    "- *Ridge regression* (using l2 norm): \n",
    "\n",
    "$$\\mathcal{L}(\\pmb{\\theta}) = \\mathrm{MSE}(\\pmb{\\theta}) + \\frac{\\lambda}{2}\\sum_{i=1}^n \\theta_i^2$$\n",
    "\n",
    "- *Lasso regression* (using l1 norm):\n",
    "\n",
    "$$\\mathcal{L}(\\pmb{\\theta}) = \\mathrm{MSE}(\\pmb{\\theta}) + \\lambda\\sum_{i=1}^n |\\theta_i|$$\n",
    "\n",
    "- $\\lambda$ is called the **regularization rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e2sm0Ftpa76"
   },
   "source": [
    "### Example: observing the effects of regularization rate\n",
    "\n",
    "(Heavily inspired by Chapter 4 of [Hands-On Machine Learning](https://github.com/ageron/handson-ml2) by Aurélien Géron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYD2dHXopa76"
   },
   "outputs": [],
   "source": [
    "m = 20\n",
    "x_reg = 3 * np.random.rand(m, 1)\n",
    "y_reg = 1 + 0.5 * x_reg + np.random.randn(m, 1) / 1.5\n",
    "x_new = np.linspace(0, 3, 100).reshape(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82yrOwlqpa7-",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_model(model_class, polynomial, lambdas, **model_kargs):\n",
    "    for reg_rate, style in zip(lambdas, (\"b-\", \"g--\", \"r:\")):\n",
    "        model = model_class(reg_rate, **model_kargs) if reg_rate > 0 else LinearRegression()\n",
    "        if polynomial:\n",
    "            model = Pipeline([\n",
    "                    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "                    (\"std_scaler\", StandardScaler()),\n",
    "                    (\"regul_reg\", model),\n",
    "                ])\n",
    "        model.fit(x_reg, y_reg)\n",
    "        y_new_regul = model.predict(x_new)\n",
    "        lw = 2 if reg_rate > 0 else 1\n",
    "        plt.plot(x_new, y_new_regul, style, linewidth=lw, label=r\"$\\lambda = {}$\".format(reg_rate))\n",
    "    plt.plot(x_reg, y_reg, \"b.\", linewidth=3)\n",
    "    plt.legend(loc=\"upper left\", fontsize=15)\n",
    "    plt.xlabel(\"$x$\", fontsize=18)\n",
    "    plt.axis([0, 3, 0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EM1xxjpypa8A",
    "outputId": "3ae3e826-31d4-4d52-ccd9-4c72369d4d71"
   },
   "outputs": [],
   "source": [
    "# Plot data and predictions with varying regularization rates\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(121)\n",
    "plot_model(Ridge, polynomial=False, lambdas=(0, 10, 100), random_state=42)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.title(\"Linear regression with Ridge regularization\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_model(Ridge, polynomial=True, lambdas=(0, 10**-5, 1), random_state=42)\n",
    "plt.title(\"Polynomial regression with Ridge regularization\", fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "colab": {
   "name": "linear_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
